# -*- coding: utf-8 -*-
"""ConfigVA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mplE_QON6D1Se-ZJE8vx6mLtLEMJ6pFI
"""



"""# <font color="00CED1">  Config Env de travail </font>   <a id="EnvTravail"></a>

### pip install
"""

# Prepa Env PIP
# librairie pour combiner Pyhton & SQL https://towardsdatascience.com/fugue-and-duckdb-fast-sql-code-in-python-e2e2dfc0f8eb
#!pip install -U fugue[duckdb,sql] 
# !pip install pandas-bokeh

#  TO DEL    !pip install -U dash 

# !python --version  # checks version from command line
# !sudo apt-get update -y
# !sudo apt-get upgrade -y

# ! pip list -v
# ! pip list -v | grep [Pp]an 
# ! pip list -v | grep [Pp]lot
# ! pip list -v | grep [Ss]ea
# ! pip list -v | grep [Bb]l
# ! pip list -v | grep [Aa]ut
# ! pip install --upgrade pandas

# !sudo update-alternatives --config python3
# #after running, enter the row number of the python version you want.
# !python --version  # checks version from command line

"""### Librairies"""

# librairies Pythons 
import numpy as np
import pandas as pd  #https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf
from pandas.core.groupby.generic import DataFrameGroupBy
pd.set_option('display.max_columns',None)  #pd.set_option('max_columns', 10) limiter le nbr de colonnes visualisé à 10 /none (=> scrollbar)
pd.set_option('display.max_rows', 40)
pd.set_option('display.max_colwidth', None)  # None ou -1 or  199
pd.set_option('display.colheader_justify','left') #'left'/'right'
# pd.set_option("display.date_dayfirst",True) # display.date_dayfirst / display.date_yearfirst
# https://pandas.pydata.org/docs/user_guide/options.html
# https://runebook.dev/fr/docs/pandas/user_guide/options

import scipy.stats as stats   # https://github.com/scipy/scipy

import statsmodels.api as sm   #https://www.statsmodels.org/stable/index.html
import statsmodels.formula.api as smf #https://www.statsmodels.org/stable/index.html
from patsy import dmatrices

import matplotlib.pyplot as plt    #https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf
import seaborn as sns     #https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Seaborn_Cheat_Sheet.pdf
import plotly.express as px     #https://plotly.com/python/ 
import plotly.graph_objects as go  #https://plotly.github.io/plotly.py-docs/search.html?q=hist&check_keywords=yes&area=default 
# import pandas_bokeh
#import fugue_duckdb
#from fugue_notebook import setup ;setup()

#_____________________________________________________________________________
# Python print Color
class pcolors:
    OK = '\033[92m' #GREEN
    WARNING = '\033[93m' #YELLOW
    FAIL = '\033[91m' #RED
    RESET = '\033[0m' #RESET COLOR

#Source : https://www.delftstack.com/fr/howto/python/python-print-colored-text/

#Exemple 
print(pcolors.OK + "File Saved Successfully!" + pcolors.RESET)
print(pcolors.WARNING + "Warning: Are you sure you want to continue?" + pcolors.RESET)
print(pcolors.FAIL + "Unable to delete record." + pcolors.RESET)

print(f"{pcolors.OK}File Saved Successfully!{pcolors.RESET}")
print(f"{pcolors.WARNING}Warning: Are you sure you want to continue?{pcolors.RESET}")
print(f"{pcolors.FAIL}Unable to delete record.{pcolors.RESET}")
#-----------------------------------------------------


#  Importer les données en Pandas dataframe
#aideDataPATH='/content/drive/MyDrive/DataSc/P4VA/'
DataPATH='https://raw.githubusercontent.com/vincent-arese/OC-P6'

"""## Fonctions Perso

#### <font color="red"> Cellules de bloc-notes template </font>

* https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf

* https://colab.research.google.com/drive/1-mfcGT9FvROM6pOL2_uNtx4ywrf6IlBn#scrollTo=icxtjOVUqN7F

* https://html-color-codes.info/Codes-couleur-HTML/

* https://docs.bokeh.org/en/latest/docs/gallery.html
"""

#########################################################################
# DEBUG & Search !      Version 1.1                                     #
#########################################################################
# dt=BilanAlim
col=['index']

# dt.info()
# dt.describe(include='all')
# dt.describe(include='category')

#~~Search NaN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# SearchNaN(dt,1)
# dt.isnull().sum().sum()  # Is nul total df 
# SearchNaN(dt,2)
# SearchNaN(dt,2).CodeZone.sort_values(na_position='first').unique()
# len(SearchNaN(dt,2).CodeZone.sort_values(na_position='first').unique())
# SearchNaN(dt,2).Zone.sort_values(na_position='first').unique()
# SearchNaN(dt,2).Année.sort_values(na_position='first').unique()

# dt.replace([np.inf, -np.inf], np.nan, inplace=True) # Remplacer les infini par des NaN (division ! )

#~~Search values ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# SearchListe(dt,[0])
# SearchListe(dt,[0,np.nan,np.inf,-np.inf])
# SearchListe(dt,[ 1, 249, 250, 273, 276, 351])
# SearchListe(dt,[0]).CodeZone
# dt.CodeZone.sort_values(ascending=True,na_position='first').unique()
# dt.iloc[:,12].sort_values(ascending=True,na_position='first').unique()
#  dt.iloc[:,2].unique()
# dt.iloc[:,3:19]
# dt.AlphaISO3.sort_values(ascending=True,na_position='first').unique()
# len(dt.AlphaISO3.sort_values(ascending=True,na_position='first'))



#__Liste valeur unique dans chaque colonne du df  _____________
# for col in ListeCol: 
#   # print(col,':',len(dt[col].unique()),': \n',dt[col].sort_values(ascending=True,na_position='first').unique(),'\n______________\n')
#   print(col,':',len(dt[col].unique()),': \n',dt[col].unique(),'\n______________\n') #Unsorted
#-----------

#~~ Différences liste ou df ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# LostElement(A_array,B_array)
# get_different_rows(source_df, new_df)
# ListeColUnique(dt)


#~~ Différences liste ou df ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# df=dt
# regex="^[Tt]h.*"
# df[df.Zone.str.contains(regex)]
#-----------


#~~ Dtype ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# 0.  # dtype Category => object (str)
# reduce_mem_usage(dt) # Optisation dtype
#__iloc dtype change ____________________
# for i in range(2,5):
#   dt.iloc[:,i]=dt.iloc[:,i].astype(float)
#-----------

#------ test Erreur dtype trop Petit: 

# print('Min',dt[col].min(),'Moy',dt[col].mean(),'Max',dt[col].max(),'Sum',dt[col].sum())  


# dt.info()

##### Df List
# SecuAlimGeo

##################### Aide memoire ##################
# [i for i in range(2,12)]

# dt.iloc[:, [2,3,12,14,15]]
# dt.iloc[:,[i for i in range(18)]],2)

# ColMove(df,ColName,ColIndex)
# df= df.iloc[:,[0,1,2,3,7,12,16]]
# o
####################################################


# print(BilanAlimGeo.Semences.max(),BilanAlimGeo.Semences.mean(), BilanAlimGeo.Semences.min(),BilanAlimGeo.Semences.sum(),BilanAlimGeo.Semences.count())
# !cat /proc/meminfo # Voir RAM

#########################################################################
# DEBUG & Compare Sum      Version 1.01                                 #
#########################################################################

df = pd.DataFrame({'num': [1, 2, 3], 'colors': ['red', 'white', 'blue']})
#_____________________________________________________________
# Parametre Compare Somme
dfREF=df
RechercheTest="num>0"
listeCol=dfREF.columns   # listeCol=dfREF.columns # pour filter toutes les colonnes de dfREF

df1=dfREF
df1TEXTE='Version1'

df2=dfREF
df2TEXTE='Version2'
# Itere  listeCol pour chaque df(n) et teste somme df(n) = somme dfREF 
#-------------------------------------------

for Col in listeCol:
  Col_type = dfREF[Col].dtype
  if Col_type != object and Col_type != "category":  #  Si le dtype != object & category => de type Nombre ! 
    if Col in df1.columns:   
      Test1=df1.query(RechercheTest)[Col].sum()
    if Col in df2.columns:
        Test2=df2.query(RechercheTest)[Col].sum()

    Ref=dfREF.query(RechercheTest)[Col].sum()
    if Test1!=Ref or Test2!=Ref:
       print(dfREF.columns.get_loc(Col),'~',Col,'~dtype(',Col_type,')   ', df1TEXTE,':',Test1==Ref,' - ',df2TEXTE,':',Test2==Ref)
    else:
      print(dfREF.columns.get_loc(Col),'~',Col,pcolors.OK +'~dtype(',Col_type,')  : Somme  Ok'+ pcolors.RESET)  # La somme  colonne est identique 
  else:
    Test1un=0
    Test2un=0
    if Col in df1.columns:   
      Test1un=df1.query(RechercheTest)[Col].unique().sum()
    if Col in df2.columns:
      Test2un=df2.query(RechercheTest)[Col].unique().sum()

    Refun=dfREF.query(RechercheTest)[Col].unique().sum()
    if Test1un !=Refun or Test2un !=Refun:
      print(dfREF.columns.get_loc(Col),'~',Col,'~dtype(',Col_type,')   ', df1TEXTE,':',Test1un==Refun,' - ',df2TEXTE,':',Test2un==Refun)
    else:
      print(dfREF.columns.get_loc(Col),'~',Col,pcolors.OK +'~dtype(',Col_type,')  : Concat OK'+ pcolors.RESET)   # La somme (textes concaténés)  colonne est identique 

#_FIN______________________________________________________

"""#### Liste Fonctions

### Search
"""

def get_df_name(df):
    name =[x for x in globals() if globals()[x] is df][0]
    return name
#source : https://stackoverflow.com/questions/31727333/get-the-name-of-a-pandas-dataframe   Pour les series utiliser .name  (ou .names )

"""##### Search NaN  SearchNaN(DataFrame,Option)"""

#Fonction ChercheNaN : Recherche des NaN
def SearchNaN(DataFrame,Option):
  """ChercheNaN : Recherche des NaN
  
  Option : 
  1: Total nombre de NaN par colonne
  2: Afficher les lignes ayant au moins 1 NaN
  3: Nomnbre de lignes ayant 1 NaN
  """
  df_name = get_df_name(DataFrame)

  if Option==1:
   return print(pcolors.OK+ df_name +pcolors.RESET,': Nombre de NaN par colonne (total lignes',len(DataFrame),'):/n' ,
                DataFrame.isnull().sum(axis=0),'\n------%-----\n',
                (DataFrame.isnull().sum(axis=0)/len(DataFrame))*100) #  Option1 : Total nombre de NaN par colonne
  elif Option==2:
    return DataFrame[DataFrame.isnull().any(axis=1)]  #Afficher les lignes ayant au moins 1 NaN
  elif Option==3: 
    return print(pcolors.OK+ df_name +pcolors.RESET,': Nombre de ligne(s) ayant au moins 1 NaN :',
                 len(DataFrame[DataFrame.isnull().any(axis=1)]),'\n-----%------\n',
                 (len(DataFrame[DataFrame.isnull().any(axis=1)])/len(DataFrame))*100)
  else:
    print("SearchNaN(DataFrame,Option) Error :","Choisir Option :\n",
          "\n 1: Total nombre de NaN par colonne",
          "\n 2: Total nombre de NaN par colonne",
          "\n 3: Nomnbre de lignes ayant 1 NaN")

# DEBUG  recherche NaN
# temp=geo
# temp.isnull().sum(axis=0) #  Option1 : Total nombre de NaN par colonne
# test=temp[temp.isnull().any(axis=1)] ; test #Afficher les lignes ayant au moins 1 NaN
# len(test)
# test.Zone.unique()

"""##### SearchListe(df,liste)"""

# Recherche une liste dans un DataFrame
def SearchListe(df,liste):
  """Recherche une liste dans un DataFrame et renvoie Df avec les Valeur pour masque SearchListe(dx,[2,4,7]) - liste=[2,4,7]/ SearchListe(dx,liste) accepte liste unique liste=[1]"""
  return df[df.isin(liste)].dropna(thresh=1)

# Exemple : 
# dx = pd.DataFrame(np.random.randint(10, size=(10, 3)),
#                      columns=['A', 'B', 'C'])
# dx
# SearchListe(dx,[2,4,7])
# liste=[2,4,7]
# SearchListe(dx,liste)
# SearchListe(dx,l)

"""##### ListeCol Unique"""

def ListeColUnique(df):
  """ Itere sur les colonnes d'un DataFrame(df) et listes les valeurs unique de chaque colonne"""
  ListeCol=df.columns
  df_name = get_df_name(df)
  print(ListeCol)
  for col in ListeCol: 
     print(col,':',len(df[col].unique()),': \n',df[col].sort_values(ascending=True,na_position='first').unique(),'\n______________\n')

"""##### SearchKey(df)"""

def SearchKey(df):
  """ Itere sur les colonnes d'un DataFrame(df) et Compte les valeurs uniques de chaque colonne"""
  ListeCol=df.columns
  df_name = get_df_name(df)
  # print('Liste colonnes :',pcolors.OK +ListeCol+ pcolors.RESET)
  print('\n______________________________________________________________________________________\n',
        "Recherche  valeurs uniques : ",
        pcolors.WARNING+ df_name +pcolors.RESET," ",
        # df.shape,
        "(",len(df),"Lignes,",len(ListeCol),"Colonnes)",
        '\n______________________________________________________________________________________')
  for col in ListeCol: 
    NbUnique=df[col].nunique()
    Nbdoublons=len(df)-NbUnique
    if Nbdoublons==0:
      print(pcolors.OK +col,":"+pcolors.RESET,len(df),"rows",
          "|Nb unique:",NbUnique,
          "|Nb doublons:", Nbdoublons, "-",
          np.round(((len(df)-NbUnique)/len(df))*100,0),'%',
          pcolors.OK+"Clé candidate"+pcolors.RESET,
          "|",df[col].dtype,
          '\n____________________________________________________________________________________')
    elif Nbdoublons!=0:
      print(pcolors.OK+col,":"+pcolors.RESET,len(df),"rows",
          "|Nb unique:",NbUnique,
          "|Nb doublons:", Nbdoublons, "-",
          np.round(((len(df)-NbUnique)/len(df))*100,0),'%', 
          "|",df[col].dtype,     
          '\n______________________________________________________________________________________')

"""##### np.Array  LostElement(Array1,Array2)"""

# Rechercher un élément perdu à partir d’un tableau dupliqué 
def LostElement(A,B):
  """Rechercher un élément perdu à partir d’un tableau dupliqué
    LostElement(ArrayA,ArrayB)  ou LostElement(df.A.unique(),df.B.unique())
    Compare deux np.array !!!!
      =>  DifférenceA=>B  : Valeur incluse dans A mais Non dans B 
      =>  DifférenceB=>A  : Valeur incluse dans B mais Non dans A
  """

  # return print('ArrayA:',A,'- Nb val:',len(A),'\n ArrayB:',B,'- Nb val:',len(B),pcolors.OK + '\n----------------------\n Différence inA=>notB',np.setdiff1d(A, B),'\n----------------------\n Différence inB=>notN',np.setdiff1d(B, A),'\n----------------------\n'+ pcolors.RESET)
  return print('\n----------------------\n',
               pcolors.OK +'Diff. inA=>notB'+ pcolors.RESET,np.setdiff1d(A, B),'\n----------------------\n',
               pcolors.OK+'Diff. inB=>notA'+ pcolors.RESET,' ',np.setdiff1d(B, A),'\n----------------------\n',
               pcolors.OK +'ArrayA:'+ pcolors.RESET,A,'- Nb val:',len(A),'\n',
               pcolors.OK +'\n ArrayB:'+ pcolors.RESET,B,'- Nb val:',len(B),'\n----------------------\n')

"""##### Différences entre deux df get_different_rows(source_df, new_df)


"""

def get_different_rows(source_df, new_df):
    """Returns just the rows from the new dataframe that differ from the source dataframe"""
    merged_df = source_df.merge(new_df, indicator=True, how='outer')
    changed_rows_df = merged_df[merged_df['_merge'] == 'right_only']
    return changed_rows_df.drop('_merge', axis=1)


#Trouver les lignes peu communes entre deux DataFrames
# pd.concat([SecuAlimGeo,SecuAlim]).drop_duplicates(keep=False)

# #Trouver les lignes peu communes entre deux DataFrames
# pd.concat([SecuAlimGeo,SecuAlim]).drop_duplicates(keep=False)

"""### Manip DataFrame

##### ColMove(df,ColName,ColIndex)
"""

# Deplacer une colonne dans un dataframe 
def ColMove(DataFrame,ColName,ColIndex):
  """Fonction  ColMove(DataFrame,ColName,ColIndex)  
   Deplace un colonne dans un DataFrame  - ColName : nom colonne  - ColIndex : rang index souhaité (ddbut df 0 !)
  """ 
  DataFrame.insert(ColIndex,'Xcol',DataFrame[ColName])
  DataFrame.drop(ColName, axis=1, inplace=True)
  DataFrame.rename(columns={'Xcol' : ColName},inplace=True)
  return DataFrame

"""### Manip Date"""

# Eclater date en colonnes  Journée - Jour - Mois - Année - Trimestre 

# data['date_naissance'] = pd.to_datetime(data['date_naissance'],format='%d/%m/%Y', errors='coerce')

#https://docs.python.org/fr/3/library/datetime.html#strftime-and-strptime-format-codes

"""### Data type Optimisation

##### Change dtype Category to Object : dtypeCat2Obj(df)
"""

def dtypeCat2Obj(df):
    """  iterate through all the columns of a dataframe and 
    modify the Dtype Catergory to Object (Str)       
    """
     
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'category':
             df[col] = df[col].astype(str)    
           
    return df

def DataChangeCat2Obj(df):
    """  iterate through all the columns of a dataframe and 
    modify the Dtype Catergory to Object (Str)       
    """
     
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'category':
             df[col] = df[col].astype(str)    
           
    return df

"""##### ReduceMemUsage reduce_mem_usage(df)"""

# Drastically reduce df RAM usage ! 
#I don't know who the original author of this function but many thanks to him ;) 
#source : https://towardsdatascience.com/how-to-learn-from-bigdata-files-on-low-memory-incremental-learning-d377282d38ff
def reduce_mem_usage(df):
    """ 
    iterate through all the columns of a dataframe and 
    modify the data type to reduce memory usage.        
    """
    start_mem = df.memory_usage().sum() / 1024**2
    print(('Memory usage of dataframe is {:.2f}' 
                     'MB').format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max <\
                  np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max <\
                   np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max <\
                   np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max <\
                   np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max <\
                   np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max <\
                   np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
        else:
            df[col] = df[col].astype('category')
    end_mem = df.memory_usage().sum() / 1024**2
    print(('Memory usage after optimization is: {:.2f}' 
                              'MB').format(end_mem))
    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) 
                                             / start_mem))
    
    return df

    # Attention ne fct pas si le df à optimizer contient un type 'category'
  #Code ...........
    # df.info(memory_usage=True)
    # DataFrame.memory_usage(index=True, deep=False)



"""###### dtype Min-Max"""

pd.DataFrame.from_dict(
{'np.int8:':[np.iinfo(np.int8).min, np.iinfo(np.int8).max],
'np.int16:':[np.iinfo(np.int16).min, np.iinfo(np.int16).max],
'np.int32:':[np.iinfo(np.int32).min, np.iinfo(np.int32).max],
'np.int64:':[np.iinfo(np.int64).min, np.iinfo(np.int64).max],
'np.float16:':[np.finfo(np.float16).min, np.finfo(np.float16).max],
'np.float32:':[np.finfo(np.float32).min, np.finfo(np.float32).max],
'np.float64:':[np.finfo(np.float64).min, np.finfo(np.float64).max]})

## info complementaire https://towardsdatascience.com/reducing-memory-usage-in-pandas-with-smaller-datatypes-b527635830af

"""##### MultiDOWN ( Conversion multi-index  à index simple)"""

#Fonction multiDOWN : Conversion df_MultiIndex to df_indexSimple
def multiDOWN(DataFrame):
  """Fonction multiDOWN(DataFrame) : Conversion DataFrame_MultiIndex to DataFrame_indexSimple"""
  DataFrame=DataFrame.columns=DataFrame.columns.to_flat_index() # colonnes => concatener le multi index en un index simple https://datascientyst.com/flatten-multiindex-in-pandas/
  DataFrame.reset_index(inplace=True) # Conversion Index en colonnes simple.
 

  # probleme  d'execution

"""### Fonction Satistiques

##### PercentCumul(df,Col) Calcul % cumulé
"""

# Tri ordre decroissant puis ajoute Colonnne avec Calcul % cumulé pour identifier 20/80 ou 80/20 
def PercentCumul(df,Col):
  """ Tri ordre decroissant puis ajoute Colonnne Col_cum% avec Calcul % cumulé pour identifier facilement 20/80 ou 80/20 """
  df=df.sort_values(by=[Col],ascending=False)
  df[Col+'_percent']=(df[Col]/df[Col].sum()) * 100
  df[Col+'cum_p']=(df[Col].cumsum() / df[Col].sum()) * 100
  df[Col+'_percent']=df[Col+'_percent'].round(2)
  df[Col+'cum_p']=df[Col+'cum_p'].round(2)
 
  # df=df.sort_values(by=[Col+'cum_%'],ascending=False)
  
  return df


# Cf cumsum() pour  SommeCumulée
 #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cumsum.html 


 # Aller plus loing  Pareto :  https://commentprogresser.com/outil-pareto.html

"""#### Moyenne Pondérée """

# Calcul Moyenne Pondérée / Weighted Average

def MoyPonderee(df, Colvalues, Colweights):
  """ Moyenne Pondérée / Weighted Average
    df: DataFrame  - Colvalues : Col Valeurs  - Colweights : Col. Poids relatif 
  Peut être utlisée avec un Groupby : df.groupby('Year').apply(MoyPonderee, 'Grades', 'NumCourses')
  Source :  Rework of https://datagy.io/pandas-weighted-average/ 
  """
  return sum(df[Colweights] * df[Colvalues]) / df[Colweights].sum()


# Source Rework of https://datagy.io/pandas-weighted-average/

"""#### QartileIndicatorCol(df,col) Ajout Colonne quartile"""

# Quartile (Ajout colonne conditionnelle quartile) & Outliers methode Interquartiles V2 

def QartileIndicatorCol(df,col): 
  """ Ajout d'un colonne Conditionelle avec le quartile au df courant  & Recherce Ouliers Methode IQ ( InterQuartiles)
QartileIndicatorCol(df,col) => df[col+"Quartile"] """

  # Caculs Seuils des quartiles 
  q25=df[col].quantile(q=0.25, interpolation='linear')  #Q1
  q50=df[col].quantile(q=0.50, interpolation='linear')  #Q2
  q75=df[col].quantile(q=0.75, interpolation='linear')  #Q3

  Q1=q25 ;   Q2=q50 ;   Q3=q75 # Quartiles simplifiés

  # Quartiles Conditions & label 
  conditionlist = [
    (df[col] <= q25) ,
    (df[col] > q25) & (df[col] <q75),
    (df[col] >= q75)]
   
  choicelist = ['quartile1','quartile2','quartile3']

  df[col+"_Quartile"] = np.select(conditionlist, choicelist, default='Not Specified')

  # écart interquartile
  EcartInterquartile  = Q3 - Q1  # IQ = Q3 - Q1
  TauxIQoutliers = 1.5
  IQoutliersInf = Q1 -  TauxIQoutliers *  EcartInterquartile # Outliers Inférieurs ?
  IQoutliersSup = Q3 +  TauxIQoutliers *  EcartInterquartile # Outliers ISuperieur ? 

 # IQ Conditions & label 
  conditionlist = [
   (df[col] <  IQoutliersInf) ,
   (df[col] >=  IQoutliersInf) & (df[col] <= IQoutliersSup),
   (df[col] > IQoutliersSup)    ]
  
  choicelist = ['IQ_Ouliers_Inf','-','IQ_Ouliers_Sup'] 

  df[col+"_IQ_Ouliers"] = np.select(conditionlist, choicelist, default='Not Specified')
  
  print( pcolors.OK + "Methode IQ "+ col+ ":"+ pcolors.RESET ,
        "Ecart Interquartile= ",EcartInterquartile,
        " -Q1 ",Q1," -Q2 ",Q2, " -Q3 ",Q3)
  return df

"""####  Zscore :  Zscore(df,Col)"""

#Zscore
def Zscore(df,Col): 
  """Zscore(df,Col) :  Ajout colonnes Zscore_Col &  NivConf_Zscore_col
                      'ZSscore Omlit NaN  & Interepretion Niv confiance
  Parametres  Df: DataFrame & col : Colonne
  """
  import scipy.stats as stats 
  #https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zscore.html
  df["Zscore_"+Col] = stats.zscore(df[Col], axis=0, nan_policy='omit')

  #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html 
  # df["ZscoreManuel"+Col] = (df[Col] - df[Col].mean())/df[Col].std(ddof=0) # Calcul Zscore Manuel
  

  Za=2.58 ;ZaConf="99%" # Seuil Intervale  confiance 99%  
  Zb=1.96 ;ZbConf="95%"# seuil Intervale  confiance95%    
  Zc=1.65 ;ZcConf="90%"# seuil Intervale confiance 90%   
  # Plus l'intervale est grand plus on s'éloigne de la moyenne et plus la proba de contenir un outlier est élevée. 
  
  """Calcul de l’intervalle de confiance : Le principe général d’un intervalle de confiance consiste à déterminer, 
  à partir de ce qui a été observé dans un sous-échantillon, un intervalle dans lequel la grandeur que l’on étudie,
  au sein de la population dont est extrait l’échantillon, a de fortes chances de se situer. En l’occurrence, 
  il s’agit de déterminer un intervalle, connaissant la proportion p observée dans l’échantillon, 
  au sein duquel la proportion π réelle de la population étudiée se situe avec une probabilité égale à une valeur fixée à l’avance, 
  usuellement 95 %, et notée 1-α."""

 
  # https://www.math.u-bordeaux.fr/~mchabano/Tab0.pdf
  # Zd=1.65 ;ZcConf="99,9%"# seuil confiance 99%   risque erreur α = 0.1%
  # source : https://pro.arcgis.com/fr/pro-app/2.7/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm
  # source2 : https://joseph.larmarange.net/?Intervalle-de-confiance-bilateral#:~:text=Le%20plus%20souvent%2C%20les%20intervalles,z%3D%201%2C95996398454%20%E2%89%88%201%2C960
  # Tables loi normale :  https://blog.univ-reunion.fr/alessioguarino/files/2016/08/Tables-Loi-Normale-test-Z-Khi2-Student.pdf
  #  np.abs(z_score) > threshold:


  # Colonne Interpration Zscore 
  ColRefName="Zscore_"+Col # Nom Colonne de référence pour la segmentation
  ColCatName="Seuil_IConF_Zscore" # Nom Colonne Categorie
  df[ColCatName]="?"
  df[ColRefName].fillna(0,inplace=True)
  
  df.loc[np.abs(df[ColRefName]) > Za , ColCatName] = "Superieur à "+ZaConf 
  df.loc[np.abs(df[ColRefName]) <= Za , ColCatName] = ZaConf    # utilisation de la Valeur Absolue (-za <= Seuil confiance <= Za )
  df.loc[np.abs(df[ColRefName]) <= Zb , ColCatName] = ZbConf
  df.loc[np.abs(df[ColRefName]) <= Zc , ColCatName] = ZcConf

  df.sort_values(ColRefName, inplace=True)


  print(ColRefName,ColCatName)
  return df

"""####  Standardisation loi Normale :  """

# Graphe loi normale
from scipy.integrate import quad
import matplotlib.pyplot as plt
import scipy.stats
import numpy as np


x_min = 0
x_max = 16

mean = 8
std = 2

x = np.linspace(x_min, x_max, 100)

y = scipy.stats.norm.pdf(x,mean,std)

plt.plot(x,y, color='coral')

plt.grid()

plt.xlim(x_min,x_max)
plt.ylim(0,0.25)

plt.title('How to plot a normal distribution in python with matplotlib',fontsize=10)

plt.xlabel('x')
plt.ylabel('Normal Distribution')

plt.savefig("normal_distribution.png")
plt.show()

# Standadisation d'une loi normale N(µ,σ)  :  Ramener un loi normale  à la la loi normale centrée réduite : N (0, 1)

def Standardisation(X,µ,σ):
  # Standardisation Formula :   
  Z=(X-µ)/σ  # Z : Zscore  X:observation   µ: moyenne   σ : Ecart type

  #https://www.youtube.com/watch?v=2tuBREK_mgE
  #https://www.youtube.com/watch?v=mtbJbDwqWLE  : 68-95-99.7 Rule (5.2)
  # Table Loi Normale Centrale reduite

def TestLoiNormale(df,Col): 
 df_name = get_df_name(df)

 print(pcolors.OK +"Moyenne :"+pcolors.RESET,df[Col].mean())
 print(pcolors.OK +"Moyenne :"+pcolors.RESET,df[Col].mean())
 print(pcolors.OK +"Mode :"+pcolors.RESET,df[Col].mode())
 print(pcolors.OK +"Variance Corrigée :"+pcolors.RESET,df[Col].var(ddof=0))
 print(pcolors.OK +"Ecart type Corrigée :"+pcolors.RESET,df[Col].std(ddof=0))
 print(pcolors.OK +"Skew :"+pcolors.RESET,df[Col].skew())
 print(pcolors.OK +"Kurtosis :"+pcolors.RESET,df[Col].kurtosis())
 df[Col].hist() ; plt.show()

 # Skewness empirique  : Mesure asymétrie Distribution
 γs = df[Col].skew()
 γs== 0 #alors la distribution est symétrique.
 γs>0 #alors la distribution est étalée à droite.
 γs<0 #alors la distribution est étalée à gauche.

 # Kurtosis empirique : Mesure Applatissement
 # L’aplatissement peut s’interpréter à la condition que la distribution soit symétrique !
 γk = df[Col].kurtosis() 
 γk==0 # alors la distribution a le même aplatissement que la distribution normale.
 γk>0 # alors elle est moins aplatie que la distribution normale : les observations sont plus concentrées.
 γk<0 # alors les observations sont moins concentrées : la distribution est plus aplati

"""#### Test Normalité :  TestNorm(data,α)"""

from numpy.random import seed
from numpy.random import poisson
from numpy.random import randn
# Test normalité d'un échantillon 


data = scipy.stats.norm.pdf(np.linspace(0, 1, 100),8,3)  #poisson(5, 100)  # randn(100)  # scipy.stats.norm.pdf(np.linspace(0, 1, 100),8,3)   #scipy.stats.norm.pdf(x,mean,std)
N = len(data) # Taille échantillon 
α=0.050 # niveau alpha 0.05 / 5% par defaut 

plt.hist(data, bins='auto')  # arguments are passed to np.histogram  plt.hist(data, bins='auto') 

#Test de Kolmogorov-Smirnov
#https://fr.wikipedia.org/wiki/Test_de_Kolmogorov-Smirnov?tableofcontents=0
# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html

# Test Non parametrique sur des variables quantitatives
#Attention test peut efficae dans les queues d e distribution  
# Plus pertinant sur les petit echantillon que Kolmogorov-Smirnov
# H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     
# H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population norlement distribué
#Interpretation du test : 
#  si la p-value est inférieure à un niveau alpha choisi (par exemple 0.05), alors l'hypothèse nulle est rejetée (i.e. il est improbable d'obtenir de telles données en supposant qu'elles soient normalement distribuées).
#  si la p-value est supérieure au niveau alpha choisi (par exemple 0.05), alors on ne doit pas rejeter l'hypothèse nulle. La valeur de la p-value alors obtenue ne présuppose en rien de la nature de la distribution des données.

TestKS = stats.kstest(data, 'norm', alternative='two-sided')  #alternative {‘two-sided’, ‘less’, ‘greater’},

print(TestKS ,TestKS[0],TestKS[1])
print(pcolors.OK +"Test normalité Kolmogorov-Smirnov:"+pcolors.RESET,TestKS )
if TestKS[1]<= α: 
  print("L'échantillon est issu d'une population normalement distribuée au risque alpha (",α*100,"%)")
else:
  print("La valeur de la p-value obtenue (",round(TestKS[1],3) ,">",α,"') ne présuppose en rien de la nature de la distribution des données.")




#The Shapiro-Wilk test is a test of normality. It is used to determine whether or not a sample comes from a normal distribution.
#https://fr.wikipedia.org/wiki/Test_de_Shapiro-Wilk
#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html

TestSW = stats.shapiro(data)
# Test Non parametrique sur des variables quantitatives
# Plus pertinant sur les petit echantillon que Kolmogorov-Smirnov
# H0 (hypothese nulle): l'échantillon est issu d'une population normalement distribuée au risque alpha     
# H1 (hypothese alternative): la p-value ne permet pas de conclure que l'échantillon est issu d'une population norlement distribué
#Interpretation du test : 
#  si la p-value est inférieure à un niveau alpha choisi (par exemple 0.05), alors l'hypothèse nulle est rejetée (i.e. il est improbable d'obtenir de telles données en supposant qu'elles soient normalement distribuées).
#  si la p-value est supérieure au niveau alpha choisi (par exemple 0.05), alors on ne doit pas rejeter l'hypothèse nulle. La valeur de la p-value alors obtenue ne présuppose en rien de la nature de la distribution des données.



print(pcolors.OK +"Test normalité Shapiro-Wilk:"+pcolors.RESET," W:", TestSW[0]," p-value:",TestSW[1],"N:",N)
if TestSW[1]<= α: 
  print("L'échantillon est issu d'une population normalement distribuée au risque alpha (",α*100,"%)")
else:
  print("La valeur de la p-value obtenue (",round(TestSW[1],3) ,">",α,"') ne présuppose en rien de la nature de la distribution des données.")


#https://fr.wikipedia.org/wiki/Test_statistique



"""###  Exports

#### Export DataFrame Excel /CSV :   ExportDF(df,file_name)
"""

def ExportDF(df,file_name):
 """ ExportXLS(df,file_name)   DataFrame et Nom du fichier  
  Format Excel :  .xlsx     ou Format CSV .csv (utF8 - separateur ;)
 le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   """
 import re
 df_name = get_df_name(df)  # DataFrame Name 
 testCSV=re.search('.csv', file_name) != None # True si file_name contient re.search # print(test)
 testXLS=re.search('.xls', file_name) != None # True si file_name contient re.search # print(test)

 # Specify the name of the file ( .xlx for Excel or .csv for text format)
 # file_name = 'Output.xlsx'
 if testCSV == True :
  df.to_csv(file_name, encoding='utf-8') # saving CSV
  print("Confirmation : ", df_name,"exporté sous :",file_name)
  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html
  
 elif testXLS ==True :
  df.to_excel(file_name) # saving the excelsheet 
  print("Confirmation : ", df_name,"exporté sous :",file_name) 
  # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_excel.html
  
 else :
   print("Erreur convertion : ", df_name,'en :',file_name," vérifier le format du fichier .xls .xlsx  ou .csv")


# Ajouter Fonction  to html : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_html.html?highlight=to_#pandas-dataframe-to-html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_clipboard.html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_xml.html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html
# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html

#test 
# ExportDF(df,"test.xlsx")

# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from matplotlib.backends.backend_pdf import PdfPages

# df = pd.DataFrame(np.random.random((10,3)), columns = ("col 1", "col 2", "col 3"))

# #https://stackoverflow.com/questions/32137396/how-do-i-plot-only-a-table-in-matplotlib
# fig, ax =plt.subplots(figsize=(12,4))
# ax.axis('tight')
# ax.axis('off')
# the_table = ax.table(cellText=df.values,colLabels=df.columns,loc='center')

# #https://stackoverflow.com/questions/4042192/reduce-left-and-right-margins-in-matplotlib-plot
# pp = PdfPages("foo.pdf")
# pp.savefig(fig, bbox_inches='tight')
# pp.close()
# Complement : https://stackoverflow.com/questions/33155776/export-pandas-dataframe-into-a-pdf-file-using-python

# import pandas as pd
# import pdfkit as pdf
# import sqlite3

# con=sqlite3.connect("baza.db")

# df=pd.read_sql_query("select * from dobit", con)
# df.to_html('/home/linux/izvestaj.html')
# nazivFajla='/home/linux/pdfPrintOut.pdf'
# pdf.from_file('/home/linux/izvestaj.html', nazivFajla)

# !pip install weasyprint
# #  Create a pandas dataframe with demo data:
# import pandas as pd
# demodata_csv = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'
# df = pd.read_csv(demodata_csv)

# # Pretty print the dataframe as an html table to a file
# intermediate_html = '/tmp/intermediate.html'
# to_html_pretty(df,intermediate_html,'Iris Data')
# # if you do not want pretty printing, just use pandas:
# # df.to_html(intermediate_html)

# # Convert the html file to a pdf file using weasyprint
# import weasyprint
# out_pdf= '/tmp/demo.pdf'
# weasyprint.HTML(intermediate_html).write_pdf(out_pdf)

# # This is the table pretty printer used above:

# def to_html_pretty(df, filename='/tmp/out.html', title=''):
#     '''
#     Write an entire dataframe to an HTML file
#     with nice formatting.
#     Thanks to @stackoverflowuser2010 for the
#     pretty printer see https://stackoverflow.com/a/47723330/362951
#     '''
#     ht = ''
#     if title != '':
#         ht += '<h2> %s </h2>\n' % title
#     ht += df.to_html(classes='wide', escape=False)

#     with open(filename, 'w') as f:
#          f.write(HTML_TEMPLATE1 + ht + HTML_TEMPLATE2)

# HTML_TEMPLATE1 = '''
# <html>
# <head>
# <style>
#   h2 {
#     text-align: center;
#     font-family: Helvetica, Arial, sans-serif;
#   }
#   table { 
#     margin-left: auto;
#     margin-right: auto;
#   }
#   table, th, td {
#     border: 1px solid black;
#     border-collapse: collapse;
#   }
#   th, td {
#     padding: 5px;
#     text-align: center;
#     font-family: Helvetica, Arial, sans-serif;
#     font-size: 90%;
#   }
#   table tbody tr:hover {
#     background-color: #dddddd;
#   }
#   .wide {
#     width: 90%; 
#   }
# </style>
# </head>
# <body>
# '''

# HTML_TEMPLATE2 = '''
# </body>
# </html>
# '''

# https://stackoverflow.com/questions/33155776/export-pandas-dataframe-into-a-pdf-file-using-python?newreg=2926175bf7ab431a87ec344c0fb5e967

"""#### Export Plolty to HTML :   ExportPlotly(fig,file_name) """

# Exporte un figure plotly (fig) au format HTML :  file_name.html
def ExportPlotly(fig,file_name):
 """ExportPlotly(fig,file_name)
 Exporte une figure plotly (fig) au format HTML (file_name.html)
 le fichier est stocké à la racine du projet (machine virtuelle pour Google Colab)   """

 file_name = file_name+".html"
 fig.write_html(file_name)
 print(file_name,"exporté dans le répertoire racine")

# Push to gihub en Test : https://gist.github.com/avullo/b8153522f015a8b908072833b95c3408

"""#### Wordcloud  A completer"""

# Libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html

# Create a list of word
text="BottleNeck BottleNeck BottleNeck  BottleNeck  BottleNeck   vin champagne AOC Bordeaux Bordeaux Bourgogne Bourgogne Gewurztraminer Grand Cru Château Turcaud Bordeaux Rouge Domaine La Croix    Beaune Vieilles Vignes Champagne Mailly Grand Cru  Mailly Grand Cru Brut Réserve Alscace Jurançon Pinot Gris Vendanges Tardives Sec Pinot Gris Champagne Gosset  Pinot Noir Domaine Huet Vouvray Saumur Blanc Clos RomanDomaine Saint-Nicolas Château de Villeneuve Saumur-Champigny Domaine Labranche Pinot Noir Sous La Tour Chambolle-Musigny 1er Cru  Wemyss Malts Single Cask Scotch Whisky Domaine de La Tour Cognac Frapin" #+Global.query("post_title != 0").post_title.values
text
# text

# # Create the wordcloud object
wordcloud = WordCloud(width=480, height=480, margin=0).generate(text)
wordcloudWhite = WordCloud(width=480, height=480, margin=0, background_color="white").generate(text)



# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()


plt.imshow(wordcloudWhite, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()

#WordCloud Tuto
#https://re-thought.com/creating-wordclouds-in-python/
#https://www.datacamp.com/tutorial/wordcloud-python 
# https://www.datacamp.com/tutorial/wordcloud-python
# https://thecleverprogrammer.com/2021/11/11/word-cloud-from-a-pandas-dataframe-in-python/